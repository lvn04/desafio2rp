{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOojj7+HPMQ0xL2+huG0uvY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lvn04/desafio2rp/blob/development/ingestion_edp_bronze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV1XInT9ISpN"
      },
      "outputs": [],
      "source": [
        "# install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n"
      ],
      "metadata": {
        "id": "rFIyvIOLKA1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "!pip install requests\n",
        "!pip install jsonlib"
      ],
      "metadata": {
        "id": "MsIYNipIKB2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e74df65-aab1-41cb-94d8-eb46508b4c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jsonlib\n",
            "  Downloading jsonlib-1.6.1.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: jsonlib\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for jsonlib (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for jsonlib\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for jsonlib\n",
            "Failed to build jsonlib\n",
            "Installing collected packages: jsonlib\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for jsonlib\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Running setup.py install for jsonlib ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
            "\u001b[31m╰─>\u001b[0m jsonlib\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init('spark-3.0.0-bin-hadoop3.2')"
      ],
      "metadata": {
        "id": "H18f9nvf4ug_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "import pyspark\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "# import apache.hadoop.fs.Path\n",
        "\n",
        "from pyspark.sql.functions import to_timestamp, to_date, date_format\n",
        "# import decimal\n",
        "# from datetime import datetime, date, timedelta\n",
        "# import uuid\n",
        "# import logging\n",
        "from pyspark.sql.types import *\n",
        "from functools import reduce\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "# from delta import *\n",
        "# import sys\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime, timedelta,date\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "# from dateutil.relativedelta import relativedelta\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "4r3XNjybKCS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciando spark\n",
        "spark = SparkSession.builder.appName(\"minhaAplicacao\").getOrCreate()"
      ],
      "metadata": {
        "id": "hfT8GI6bKChL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parametros Github\n",
        "GIT_USERNAME = \"lvn04\"\n",
        "GIT_TOKEN = \"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\"\n",
        "GIT_REPOSITORY = \"desafio2rp\"\n",
        "GIT_BRANCH_NAME = 'main'"
      ],
      "metadata": {
        "id": "2bpRuRGuRO0t",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clonando Repositório do GITHUB no Colab\n",
        "\n",
        "GIT_PATH = f'https://{GIT_USERNAME}:{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git'\n",
        "!mkdir ./temp\n",
        "!git clone \"{GIT_PATH}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R67eGTKqMhcs",
        "outputId": "22f71b96-0c70-4403-c89a-076524499b87",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'desafio2rp'...\n",
            "remote: Enumerating objects: 1262, done.\u001b[K\n",
            "remote: Counting objects: 100% (1262/1262), done.\u001b[K\n",
            "remote: Compressing objects: 100% (169/169), done.\u001b[K\n",
            "remote: Total 1262 (delta 513), reused 1250 (delta 504), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1262/1262), 3.75 MiB | 2.57 MiB/s, done.\n",
            "Resolving deltas: 100% (513/513), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------------\n",
        "# Pegando pasta local de trabalho\n",
        "pasta_local = os.getcwd()\n",
        "# ------------------------------------------------------------------------------------------\n",
        "# Lendo df para pegar ultima data de carga\n",
        "LOCAL = f'{pasta_local}/desafio2rp/bronze/english-prescribing-data-epd/*'\n",
        "df = (spark.read.format('parquet').load(LOCAL))\n",
        "data_delta = df.select(max('DATA_LOAD')).collect()[0][0]\n",
        "\n",
        "# ------------------------------------------------------------------------------------------\n",
        "# Setando data inicial e data final\n",
        "\n",
        "# prim_ds = date(2022,9,1)\n",
        "prim_ds = data_delta + relativedelta(months=1)\n",
        "\n",
        "# ult_ds = date(2022,10,1)\n",
        "ult_ds = (date.today()).replace(day=1)"
      ],
      "metadata": {
        "id": "JxTzRO7MXlyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "while prim_ds < ult_ds:\n",
        "  data = []\n",
        "  columns_df = []\n",
        "  data_df = []\n",
        "\n",
        "  data_referencia = prim_ds\n",
        "  mes_ano = data_referencia.strftime('%m/%Y')\n",
        "  data_ref = data_referencia.strftime('%Y%m')\n",
        "  data_load = data_referencia.strftime('%Y-%m-%d')\n",
        "\n",
        "  # link para acessar os dados da API\n",
        "  url = f\"https://opendata.nhsbsa.net/api/3/action/datastore_search_sql?resource_id=EPD_{data_ref}&sql=SELECT * from `EPD_{data_ref}` limit 30000\"\n",
        "\n",
        "  # Pegando dados com requests\n",
        "  req = requests.get(url)\n",
        "\n",
        "  if req.status_code == 200:\n",
        "      data = req.content\n",
        "      data_edp = json.loads(data)\n",
        "      print(f'Solicitação OK - \"EPD_{data_ref}\"')\n",
        "      \n",
        "     \n",
        "  elif req.status_code == 404:\n",
        "      print(f'Dataset \"EPD_{data_ref}\" não encontrado')\n",
        "      break\n",
        "\n",
        "  else:\n",
        "      print(f'Erro de solicitação - \"EPD_{data_ref}\"')\n",
        "      break\n",
        "\n",
        "  if data_edp['success'] == True:\n",
        "\n",
        "     # Separando os dados das colunas \n",
        "      for reg in data_edp['result']['result']['records']:\n",
        "          data_df.append(tuple(reg.values()))\n",
        "          columns_df.append(tuple(reg.keys()))\n",
        "\n",
        "      columns_df = set(columns_df)\n",
        "\n",
        "      list_schema = []\n",
        "      for conj_column in columns_df:\n",
        "          for column in conj_column:\n",
        "              list_schema.append(StructField(f\"{column}\",StringType(),True))\n",
        "\n",
        "      schema = StructType(list_schema)\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "      # Criando DataFrame\n",
        "      df_edp = spark.createDataFrame(data=data_df,schema=schema)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "\n",
        "      # Criando colunas de partição e data de carga\n",
        "      df_edp = (df_edp.withColumn('PART', col('YEAR_MONTH'))\n",
        "      .withColumn('DATA_LOAD', lit(data_load).cast('date'))\n",
        "      )\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "      # Selecionando as colunas\n",
        "\n",
        "      df_edp =(\n",
        "          df_edp.select(\n",
        "          'BNF_CODE',\n",
        "          'TOTAL_QUANTITY',\n",
        "          'POSTCODE',\n",
        "          'YEAR_MONTH',\n",
        "          'UNIDENTIFIED',\n",
        "          'PRACTICE_NAME',\n",
        "          'ICB_NAME',\n",
        "          'BNF_CHAPTER_PLUS_CODE',\n",
        "          'ICB_CODE',\n",
        "          'ACTUAL_COST',\n",
        "          'QUANTITY',\n",
        "          'REGIONAL_OFFICE_CODE',\n",
        "          'ITEMS',\n",
        "          'ADDRESS_4',\n",
        "          'ADDRESS_1',\n",
        "          'ADDRESS_2',\n",
        "          'ADDRESS_3',\n",
        "          'BNF_CHEMICAL_SUBSTANCE',\n",
        "          'ADQUSAGE',\n",
        "          'PCO_CODE',\n",
        "          'REGIONAL_OFFICE_NAME',\n",
        "          'NIC',\n",
        "          'CHEMICAL_SUBSTANCE_BNF_DESCR',\n",
        "          'PRACTICE_CODE',\n",
        "          'PCO_NAME',\n",
        "          'BNF_DESCRIPTION',\n",
        "          'DATA_LOAD',\n",
        "          'PART'\n",
        "          )\n",
        "      )\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    # Escrevendo dados na bronze\n",
        "      \n",
        "      DEFAULT_FOLDER = f'{pasta_local}/desafio2rp/bronze/english-prescribing-data-epd/'\n",
        "\n",
        "      LOCATION = f'{DEFAULT_FOLDER}'\n",
        "\n",
        "      (df_edp\n",
        "        .write\n",
        "        .partitionBy(\"part\")\n",
        "        .mode(\"append\")\n",
        "        # .mode('overwrite')\n",
        "        .parquet(LOCATION)\n",
        "      ) \n",
        "        \n",
        "      print(f'Dados de {mes_ano} OK')\n",
        "      prim_ds += relativedelta(months=1)\n",
        "      # break\n",
        "  else:\n",
        "      print('Erro ao pegar os dados')\n",
        "      break"
      ],
      "metadata": {
        "id": "A0rLJsRgB-iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a724f4-4921-48e4-e776-cdc332cc5684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solicitação OK - \"EPD_202209\"\n",
            "Dados de 09/2022 OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Commitando dados no GITHUB 1/2\n",
        "\n",
        "time.sleep(10)\n",
        "\n",
        "%cd \"{DEFAULT_FOLDER}\"\n",
        "\n",
        "!git config user.email \"lsouzaviana72@gmail.com\"\n",
        "!git config user.name \"Lucas Viana\""
      ],
      "metadata": {
        "id": "pJAuDRDt7LfH",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a540cb-d5df-4691-9318-ad71fe6f3a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/desafio2rp/bronze/english-prescribing-data-epd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Commitando dados no GITHUB 2/2\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "!git remote set-url origin \"{GIT_PATH}\"\n",
        "\n",
        "!git add  -A \n",
        "!git commit -m \"Novos Arquivos\"\n",
        "\n",
        "# !git push origin \"{GIT_BRANCH_NAME}\"\n",
        "!git push -u -f origin \"{GIT_BRANCH_NAME}\""
      ],
      "metadata": {
        "id": "pY-E4ebmz1cV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}